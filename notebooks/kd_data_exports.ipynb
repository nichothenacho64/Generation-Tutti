{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba750a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import sys\n",
    "from typing import Any\n",
    "\n",
    "sys.path.append(os.path.abspath(\"..\"))\n",
    "\n",
    "from eda.parsing import Conversations, Participants\n",
    "from eda.utils import FOLDER_DIR\n",
    "\n",
    "DATA_PATH = FOLDER_DIR / \"data\"\n",
    "\n",
    "def export_json_data(name: str, data: dict[str, Any]):\n",
    "    with DATA_PATH.joinpath(name).open(\"w\") as fp:\n",
    "        json.dump(data, fp, indent=4, ensure_ascii=False)\n",
    "\n",
    "\n",
    "def create_json_data() -> dict[str, Any]:\n",
    "    return {\"metadata\": {}, \"data\": {}}\n",
    "\n",
    "participants = Participants()\n",
    "conversations = Conversations(participants)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17108b93",
   "metadata": {},
   "source": [
    "### Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "81f5cb02",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversations.read_all(parallel=True, load_sentiments=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5bcca77c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from collections.abc import Iterable\n",
    "\n",
    "from eda.models import ConversationLine, Generation\n",
    "from eda.sentiments import SentimentType\n",
    "from eda.utils import round_precise\n",
    "\n",
    "\n",
    "def sentiments_from_lines(\n",
    "    lines: Iterable[ConversationLine], exclude_true_neutrals: bool = False\n",
    ") -> tuple[list[str], list[float]]:\n",
    "    sentiments = Counter()\n",
    "\n",
    "    for line in lines:\n",
    "        if not line.sentiments.has_loaded_scores():\n",
    "            continue\n",
    "        if not line.sentiments.has_scores():\n",
    "            continue\n",
    "        if exclude_true_neutrals and line.sentiments.neutral == 1.0:\n",
    "            continue\n",
    "        sentiments += line.sentiments.score_counts\n",
    "\n",
    "    total_sum = sum(sentiments.values())\n",
    "    names, scores = zip(\n",
    "        *(\n",
    "            (\n",
    "                SentimentType(sentiment_name).display_name,\n",
    "                round_precise(score / total_sum * 100, 2),\n",
    "            )\n",
    "            for sentiment_name, score in sentiments.items()\n",
    "        )\n",
    "    )\n",
    "    return list(names), list(scores)\n",
    "\n",
    "\n",
    "def sentiment_percentages() -> list[dict[str, Any]]:\n",
    "    lines_by_generation = Generation.create_mapping()\n",
    "    for conversation in conversations:\n",
    "        for line in conversation.lines:\n",
    "            generation = line.participant.generation\n",
    "            lines_by_generation[generation].append(line)\n",
    "\n",
    "    sentiment_names = None\n",
    "    generation_names = []\n",
    "    data = []\n",
    "\n",
    "    for generation, lines in lines_by_generation.items():\n",
    "        generation_names.append(generation.name)\n",
    "        sentiment_names, scores = sentiments_from_lines(\n",
    "            lines, exclude_true_neutrals=True\n",
    "        )\n",
    "\n",
    "        total = sum(scores)\n",
    "        proportions = [\n",
    "            (sentiment_name, score / total * 100)\n",
    "            for sentiment_name, score in zip(sentiment_names, scores)\n",
    "        ]\n",
    "        \n",
    "        data.append({generation.name: dict(proportions)})\n",
    "\n",
    "    return data\n",
    "\n",
    "data = create_json_data()\n",
    "data[\"metadata\"][\"title\"] = \"Sentiment percentages per generation\"\n",
    "data[\"data\"] = sentiment_percentages()\n",
    "export_json_data(\"sentiment_percentages.json\", data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "97be9240",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversations.read_all(parallel=True, load_prosodic=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7cfb0ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from collections import defaultdict\n",
    "from collections.abc import Generator\n",
    "\n",
    "from eda.models import Participant\n",
    "\n",
    "\n",
    "def human_name_from_snake_case(name: str) -> str:\n",
    "    return \" \".join(name.split(\"_\")).capitalize()\n",
    "\n",
    "\n",
    "def filter_prosodic_attributes(\n",
    "    line: ConversationLine,\n",
    ") -> Generator[tuple[str, str]]:\n",
    "    yield from (\n",
    "        (name, value) for name, value in vars(line).items() \n",
    "        if name.endswith(\"phrases\")\n",
    "        and name != \"overlapping_phrases\"\n",
    "    )\n",
    "\n",
    "\n",
    "def prosodic_frequencies(participant: Participant) -> dict[str, float]:\n",
    "    participant_lines = conversations.participant_lines(participant)\n",
    "    participant_data = defaultdict(int)\n",
    "    n_lines = 0\n",
    "    for line in participant_lines:\n",
    "        for name, value in filter_prosodic_attributes(line):\n",
    "            participant_data[name] += len(value)\n",
    "        n_lines += 1\n",
    "\n",
    "    norm_participant_data = {\n",
    "        key: value / n_lines for key, value in participant_data.items()\n",
    "    }\n",
    "    return norm_participant_data\n",
    "\n",
    "\n",
    "def prosodic_counts_by_generation(data: dict[Generation, Any]) -> dict[str, dict[str, float]]:\n",
    "    for participant in participants:\n",
    "        generation = participant.generation\n",
    "        data[generation].append(prosodic_frequencies(participant))\n",
    "\n",
    "    result = {}\n",
    "    for generation, participants_frequencies in data.items():\n",
    "        total_generation_counts = Counter()\n",
    "        for participant_frequencies in participants_frequencies:\n",
    "            counts = Counter(participant_frequencies)\n",
    "            total_generation_counts += counts\n",
    "\n",
    "        result[generation.name] = {\n",
    "            human_name_from_snake_case(prosodic_feature): count\n",
    "            / len(participants_frequencies)\n",
    "            for prosodic_feature, count in total_generation_counts.items()\n",
    "        }\n",
    "    return result\n",
    "\n",
    "\n",
    "generation_map = Generation.create_mapping()\n",
    "counts = prosodic_counts_by_generation(generation_map)\n",
    "data = create_json_data()\n",
    "data[\"metadata\"][\"title\"] = \"Average prosodic feature frequency per line by generation\"\n",
    "data[\"data\"] = counts\n",
    "export_json_data(\"prosodic_features.json\", data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d68d7f",
   "metadata": {},
   "source": [
    "### Question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d71f11d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversations.read_all(parallel=True, load_tagged=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "392beff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Optional\n",
    "\n",
    "from eda.language import TaggedText\n",
    "\n",
    "TOP_N_LEMMAS = 20\n",
    "ALLOWED_POS_VALUES = None\n",
    "PER_WORDS = 1000\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class PosTaggedLemma:\n",
    "    lemma: str\n",
    "    pos_name: str\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class GenerationLemmaInfo:\n",
    "    pt_lemmas: list[PosTaggedLemma]\n",
    "    lemma_counts: list[int]\n",
    "    n_total_words: int\n",
    "\n",
    "\n",
    "def top_lemmas_by_generation(\n",
    "    conversations,\n",
    "    *,\n",
    "    top_n: int = 10,\n",
    "    min_lemma_length: int = 3,\n",
    "    allowed_pos_values: Optional[set] = None,\n",
    ") -> dict[Generation, GenerationLemmaInfo]:\n",
    "    text_by_generation: dict[Generation, list[TaggedText]] = Generation.create_mapping()\n",
    "\n",
    "    for conversation in conversations:\n",
    "        for line in conversation:\n",
    "            generation = line.participant.generation\n",
    "            text_by_generation[generation].extend(line.tagged)\n",
    "\n",
    "    lemmas_by_generation = {}\n",
    "    for generation, words in text_by_generation.items():\n",
    "        words_by_lemma = defaultdict(list)\n",
    "        for word in words:\n",
    "            if len(word.lemma) < min_lemma_length:\n",
    "                continue\n",
    "            if allowed_pos_values is not None and word.pos not in allowed_pos_values:\n",
    "                continue\n",
    "            pt_lemma = PosTaggedLemma(word.lemma, word.pos_name)\n",
    "            words_by_lemma[pt_lemma].append(word)\n",
    "\n",
    "        counts = Counter({\n",
    "            pt_lemma: len(group) for pt_lemma, group in words_by_lemma.items()\n",
    "        })\n",
    "\n",
    "        lemmas, frequencies = zip(*counts.most_common(top_n))\n",
    "        lemmas_by_generation[generation] = GenerationLemmaInfo(\n",
    "            list(lemmas), list(frequencies), len(words)\n",
    "        )\n",
    "\n",
    "    return lemmas_by_generation\n",
    "\n",
    "def important_lemmas() -> dict[str, Any]:\n",
    "    top_lemmas = top_lemmas_by_generation(\n",
    "        conversations, top_n=TOP_N_LEMMAS, allowed_pos_values=ALLOWED_POS_VALUES\n",
    "    )\n",
    "\n",
    "    result = {key.name: {} for key in Generation.create_mapping()}\n",
    "    for generation, info in top_lemmas.items():\n",
    "        pt_lemmas = info.pt_lemmas\n",
    "        counts = info.lemma_counts\n",
    "        gen_name = generation.name\n",
    "        for ptl, count in zip(pt_lemmas, counts):\n",
    "            lemma_frequency = round_precise((count / info.n_total_words) * PER_WORDS, 2)\n",
    "            result[gen_name][ptl.lemma] = lemma_frequency\n",
    "    return result\n",
    "\n",
    "\n",
    "data = create_json_data()\n",
    "data[\"metadata\"][\"title\"] = \"Top lemmas by generation\"\n",
    "data[\"metadata\"][\"top_n_lemmas\"] = TOP_N_LEMMAS\n",
    "data[\"metadata\"][\"per_n_words\"] = PER_WORDS\n",
    "data[\"data\"] = important_lemmas()\n",
    "export_json_data(\"top_lemmas.json\", data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d08f9a71",
   "metadata": {},
   "source": [
    "### Question 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb11af82",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from collections.abc import Callable\n",
    "from functools import cache\n",
    "\n",
    "from eda.language import AttributedWord\n",
    "from eda.models import MacroRegion, ParticipantLines\n",
    "\n",
    "\n",
    "def participant_macro_region(participant: Participant) -> MacroRegion:\n",
    "    conversation = conversations.conversation(participant.conversation_code)\n",
    "    return conversation.macro_region\n",
    "\n",
    "\n",
    "def generate_participant_words(lines: ParticipantLines) -> Generator[AttributedWord]:\n",
    "    for line in lines:\n",
    "        yield from filter(lambda word: word.is_linguistic, line.normalised_words)\n",
    "\n",
    "@cache\n",
    "def participants_dialect_percentages(\n",
    "    *, rounder: Callable[[int | float, int], int | float] = round_precise\n",
    ") -> list[int | float]:\n",
    "    percentages = []\n",
    "\n",
    "    for participant in participants:\n",
    "        participant_lines = conversations.participant_lines(participant)\n",
    "        dialect_words = total_words = 0\n",
    "        for word in generate_participant_words(participant_lines):\n",
    "            dialect_words += word.is_dialect(strict=False)\n",
    "            total_words += 1\n",
    "\n",
    "        percentange_of_dialect_words = rounder(dialect_words / total_words * 100, 2)\n",
    "        percentages.append(percentange_of_dialect_words)\n",
    "\n",
    "    return percentages\n",
    "\n",
    "def labeled_percentages() -> list[dict[str, Any]]:\n",
    "    result = {\n",
    "        key.name: value \n",
    "        for key, value in Generation.create_mapping().items()\n",
    "    }\n",
    "    \n",
    "    region_percentages = defaultdict(float)\n",
    "\n",
    "    dialect_percentages = iter(participants_dialect_percentages())\n",
    "    for participant in participants:\n",
    "        region = participant.geographic_origin\n",
    "        percentage = next(dialect_percentages)\n",
    "        participant_data = {\n",
    "            \"region\": region,\n",
    "            \"dialect_percentage\": percentage,\n",
    "            \"macro_region\": participant.macro_region.name.lower()\n",
    "        }\n",
    "        region_percentages[region] += percentage\n",
    "        result[participant.generation.name].append(participant_data)\n",
    "    return result  # type: ignore\n",
    "\n",
    "data = create_json_data()\n",
    "data[\"metadata\"][\"title\"] = \"Dialect word percentages\"\n",
    "data[\"data\"] = labeled_percentages()\n",
    "export_json_data(\"dialect_percentages.json\", data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
