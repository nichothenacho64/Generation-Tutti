{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba750a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import sys\n",
    "from typing import Any\n",
    "\n",
    "sys.path.append(os.path.abspath(\"..\"))\n",
    "\n",
    "from eda.models import Generation\n",
    "from eda.parsing import Conversations, Participants\n",
    "from eda.utils import FOLDER_DIR\n",
    "\n",
    "DATA_PATH = FOLDER_DIR / \"data\"\n",
    "\n",
    "def export_json_data(name: str, data: dict[str, Any]):\n",
    "    with DATA_PATH.joinpath(name).open(\"w\") as fp:\n",
    "        json.dump(data, fp, indent=4, ensure_ascii=False)\n",
    "\n",
    "\n",
    "def create_json_data() -> dict[str, Any]:\n",
    "    return {\"metadata\": {}, \"data\": {}}\n",
    "\n",
    "participants = Participants()\n",
    "conversations = Conversations(participants)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17108b93",
   "metadata": {},
   "source": [
    "### Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "81f5cb02",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversations.read_all(parallel=True, load_sentiments=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5bcca77c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from collections.abc import Iterable\n",
    "\n",
    "from eda.models import ConversationLine\n",
    "from eda.sentiments import SentimentType\n",
    "from eda.utils import round_precise\n",
    "\n",
    "\n",
    "def sentiments_from_lines(\n",
    "    lines: Iterable[ConversationLine], exclude_true_neutrals: bool = False\n",
    ") -> tuple[list[str], list[float]]:\n",
    "    sentiments = Counter()\n",
    "\n",
    "    for line in lines:\n",
    "        if not line.sentiments.has_loaded_scores():\n",
    "            continue\n",
    "        if not line.sentiments.has_scores():\n",
    "            continue\n",
    "        if exclude_true_neutrals and line.sentiments.neutral == 1.0:\n",
    "            continue\n",
    "        sentiments += line.sentiments.score_counts\n",
    "\n",
    "    total_sum = sum(sentiments.values())\n",
    "    names, scores = zip(\n",
    "        *(\n",
    "            (\n",
    "                SentimentType(sentiment_name).display_name,\n",
    "                round_precise(score / total_sum * 100, 2),\n",
    "            )\n",
    "            for sentiment_name, score in sentiments.items()\n",
    "        )\n",
    "    )\n",
    "    return list(names), list(scores)\n",
    "\n",
    "\n",
    "def sentiment_percentages() -> dict[str, dict[str, Any]]:\n",
    "    lines_by_generation = Generation.create_mapping()\n",
    "    for conversation in conversations:\n",
    "        for line in conversation.lines:\n",
    "            generation = line.participant.generation\n",
    "            lines_by_generation[generation].append(line)\n",
    "\n",
    "    sentiment_names = None\n",
    "    generation_names = []\n",
    "    data = {}\n",
    "\n",
    "    for generation, lines in lines_by_generation.items():\n",
    "        generation_names.append(generation.name)\n",
    "        sentiment_names, scores = sentiments_from_lines(\n",
    "            lines, exclude_true_neutrals=True\n",
    "        )\n",
    "\n",
    "        total = sum(scores)\n",
    "        proportions = [\n",
    "            (sentiment_name, score / total * 100)\n",
    "            for sentiment_name, score in zip(sentiment_names, scores)\n",
    "        ]\n",
    "        \n",
    "        data[generation.name] = dict(proportions)\n",
    "\n",
    "    return data\n",
    "\n",
    "data = create_json_data()\n",
    "data[\"metadata\"][\"title\"] = \"Sentiment percentages per generation\"\n",
    "data[\"data\"] = sentiment_percentages()\n",
    "export_json_data(\"sentiment_percentages.json\", data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "97be9240",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversations.read_all(parallel=True, load_prosodic=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cfb0ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from collections import defaultdict\n",
    "from collections.abc import Generator\n",
    "\n",
    "from eda.models import Participant\n",
    "\n",
    "\n",
    "def human_name_from_snake_case(name: str) -> str:\n",
    "    return \" \".join(name.split(\"_\")).capitalize()\n",
    "\n",
    "\n",
    "def filter_prosodic_attributes(\n",
    "    line: ConversationLine,\n",
    ") -> Generator[tuple[str, str]]:\n",
    "    yield from (\n",
    "        (name, value) for name, value in vars(line).items() \n",
    "        if name.endswith(\"phrases\")\n",
    "        and name != \"overlapping_phrases\"\n",
    "    )\n",
    "\n",
    "\n",
    "def prosodic_frequencies(participant: Participant) -> dict[str, float]:\n",
    "    participant_lines = conversations.participant_lines(participant)\n",
    "    participant_data = defaultdict(int)\n",
    "    n_lines = 0\n",
    "    for line in participant_lines:\n",
    "        for name, value in filter_prosodic_attributes(line):\n",
    "            participant_data[name] += len(value)\n",
    "        n_lines += 1\n",
    "\n",
    "    norm_participant_data = {\n",
    "        key: value / n_lines for key, value in participant_data.items()\n",
    "    }\n",
    "    return norm_participant_data\n",
    "\n",
    "\n",
    "def prosodic_counts_by_generation(data: dict[Generation, Any]) -> dict[str, dict[str, float]]:\n",
    "    for participant in participants:\n",
    "        generation = participant.generation\n",
    "        data[generation].append(prosodic_frequencies(participant))\n",
    "\n",
    "    result = {}\n",
    "    for generation, participants_frequencies in data.items():\n",
    "        total_generation_counts = Counter()\n",
    "        for participant_frequencies in participants_frequencies:\n",
    "            total_generation_counts += Counter(participant_frequencies)\n",
    "\n",
    "        result[generation.name] = {\n",
    "            human_name_from_snake_case(prosodic_feature): count\n",
    "            / len(participants_frequencies)\n",
    "            for prosodic_feature, count in total_generation_counts.items()\n",
    "        }\n",
    "    return result\n",
    "\n",
    "\n",
    "generation_map = Generation.create_mapping()\n",
    "counts = prosodic_counts_by_generation(generation_map)\n",
    "data = create_json_data()\n",
    "data[\"metadata\"][\"title\"] = \"Average prosodic feature frequency per line by generation\"\n",
    "data[\"data\"] = counts\n",
    "export_json_data(\"prosodic_features.json\", data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d68d7f",
   "metadata": {},
   "source": [
    "### Question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d71f11d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversations.read_all(parallel=True, load_tagged=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "392beff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Optional\n",
    "\n",
    "from eda.language import TaggedText\n",
    "\n",
    "TOP_N_LEMMAS = 20\n",
    "ALLOWED_POS_VALUES = None\n",
    "PER_WORDS = 1000\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class PosTaggedLemma:\n",
    "    lemma: str\n",
    "    pos_name: str\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class GenerationLemmaInfo:\n",
    "    pt_lemmas: list[PosTaggedLemma]\n",
    "    lemma_counts: list[int]\n",
    "    n_total_words: int\n",
    "\n",
    "\n",
    "def top_lemmas_by_generation(\n",
    "    conversations,\n",
    "    *,\n",
    "    top_n: int = 10,\n",
    "    min_lemma_length: int = 3,\n",
    "    allowed_pos_values: Optional[set] = None,\n",
    ") -> dict[Generation, GenerationLemmaInfo]:\n",
    "    text_by_generation: dict[Generation, list[TaggedText]] = Generation.create_mapping()\n",
    "\n",
    "    for conversation in conversations:\n",
    "        for line in conversation:\n",
    "            generation = line.participant.generation\n",
    "            text_by_generation[generation].extend(line.tagged)\n",
    "\n",
    "    lemmas_by_generation = {}\n",
    "    for generation, words in text_by_generation.items():\n",
    "        words_by_lemma = defaultdict(list)\n",
    "        for word in words:\n",
    "            if len(word.lemma) < min_lemma_length:\n",
    "                continue\n",
    "            if allowed_pos_values is not None and word.pos not in allowed_pos_values:\n",
    "                continue\n",
    "            pt_lemma = PosTaggedLemma(word.lemma, word.pos_name)\n",
    "            words_by_lemma[pt_lemma].append(word)\n",
    "\n",
    "        counts = Counter({\n",
    "            pt_lemma: len(group) for pt_lemma, group in words_by_lemma.items()\n",
    "        })\n",
    "\n",
    "        lemmas, frequencies = zip(*counts.most_common(top_n))\n",
    "        lemmas_by_generation[generation] = GenerationLemmaInfo(\n",
    "            list(lemmas), list(frequencies), len(words)\n",
    "        )\n",
    "\n",
    "    return lemmas_by_generation\n",
    "\n",
    "def important_lemmas() -> dict[str, Any]:\n",
    "    top_lemmas = top_lemmas_by_generation(\n",
    "        conversations, top_n=TOP_N_LEMMAS, allowed_pos_values=ALLOWED_POS_VALUES\n",
    "    )\n",
    "\n",
    "    result = {key.name: {} for key in Generation.create_mapping()}\n",
    "    for generation, info in top_lemmas.items():\n",
    "        pt_lemmas = info.pt_lemmas\n",
    "        counts = info.lemma_counts\n",
    "        gen_name = generation.name\n",
    "        for ptl, count in zip(pt_lemmas, counts):\n",
    "            lemma_frequency = round_precise((count / info.n_total_words) * PER_WORDS, 2)\n",
    "            result[gen_name][ptl.lemma] = lemma_frequency\n",
    "    return result\n",
    "\n",
    "\n",
    "data = create_json_data()\n",
    "data[\"metadata\"][\"title\"] = \"Top lemmas by generation\"\n",
    "data[\"metadata\"][\"top_n_lemmas\"] = TOP_N_LEMMAS\n",
    "data[\"metadata\"][\"per_n_words\"] = PER_WORDS\n",
    "data[\"data\"] = important_lemmas()\n",
    "export_json_data(\"top_lemmas.json\", data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d08f9a71",
   "metadata": {},
   "source": [
    "### Question 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fb11af82",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from collections.abc import Callable, Generator\n",
    "from functools import cache\n",
    "\n",
    "from eda.language import AttributedWord\n",
    "from eda.models import MacroRegion, ParticipantLines\n",
    "\n",
    "\n",
    "def participant_macro_region(participant: Participant) -> MacroRegion:\n",
    "    conversation = conversations.conversation(participant.conversation_code)\n",
    "    return conversation.macro_region\n",
    "\n",
    "\n",
    "def generate_participant_words(lines: ParticipantLines) -> Generator[AttributedWord]:\n",
    "    for line in lines:\n",
    "        yield from filter(lambda word: word.is_linguistic, line.normalised_words)\n",
    "\n",
    "@cache\n",
    "def participants_dialect_percentages(\n",
    "    *, rounder: Callable[[int | float, int], int | float] = round_precise\n",
    ") -> list[int | float]:\n",
    "    percentages = []\n",
    "\n",
    "    for participant in participants:\n",
    "        participant_lines = conversations.participant_lines(participant)\n",
    "        dialect_words = total_words = 0\n",
    "        for word in generate_participant_words(participant_lines):\n",
    "            dialect_words += word.is_dialect(strict=False)\n",
    "            total_words += 1\n",
    "\n",
    "        percentange_of_dialect_words = rounder(dialect_words / total_words * 100, 2)\n",
    "        percentages.append(percentange_of_dialect_words)\n",
    "\n",
    "    return percentages\n",
    "\n",
    "def labeled_percentages() -> list[dict[str, Any]]:\n",
    "    result = {\n",
    "        key.name: value \n",
    "        for key, value in Generation.create_mapping().items()\n",
    "    }\n",
    "    \n",
    "    region_percentages = defaultdict(float)\n",
    "\n",
    "    dialect_percentages = iter(participants_dialect_percentages())\n",
    "    for participant in participants:\n",
    "        region = participant.geographic_origin\n",
    "        percentage = next(dialect_percentages)\n",
    "        participant_data = {\n",
    "            \"region\": region,\n",
    "            \"dialect_percentage\": percentage,\n",
    "            \"macro_region\": participant.macro_region.name.lower()\n",
    "        }\n",
    "        region_percentages[region] += percentage\n",
    "        result[participant.generation.name].append(participant_data)\n",
    "    return result  # type: ignore\n",
    "\n",
    "data = create_json_data()\n",
    "data[\"metadata\"][\"title\"] = \"Dialect word percentages\"\n",
    "data[\"data\"] = labeled_percentages()\n",
    "export_json_data(\"dialect_percentages.json\", data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9303c8c5",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Any' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 48\u001b[39m\n\u001b[32m     45\u001b[39m     region_dialects_df = region_dialects_df.loc[:, (region_dialects_df != \u001b[32m0\u001b[39m).sum() >= \u001b[32m2\u001b[39m]\n\u001b[32m     46\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m region_dialects_df\n\u001b[32m---> \u001b[39m\u001b[32m48\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgeneration_grouped_percentages\u001b[39m(region_dialects_df: pd.DataFrame) -> \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, \u001b[43mAny\u001b[49m]:\n\u001b[32m     49\u001b[39m     percentages = {\n\u001b[32m     50\u001b[39m         generation.name: {}\n\u001b[32m     51\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m generation \u001b[38;5;129;01min\u001b[39;00m Generation.create_mapping()\n\u001b[32m     52\u001b[39m     }\n\u001b[32m     54\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m i, row \u001b[38;5;129;01min\u001b[39;00m region_dialects_df.iterrows():\n",
      "\u001b[31mNameError\u001b[39m: name 'Any' is not defined"
     ]
    }
   ],
   "source": [
    "from functools import partial\n",
    "from typing import Optional, cast\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def generational_dialect_percentages(top_n: Optional[int] = None) -> pd.DataFrame:\n",
    "    data = []\n",
    "    region_percentages = defaultdict(float)\n",
    "\n",
    "    dialect_percentages = iter(participants_dialect_percentages())\n",
    "    for participant in participants:\n",
    "        region = participant.geographic_origin\n",
    "        percentage = next(dialect_percentages)\n",
    "        participant_data = {\n",
    "            \"generation\": participant.generation.name,\n",
    "            \"dialect_percentage\": percentage,\n",
    "            \"region\": region\n",
    "        }\n",
    "        region_percentages[region] += percentage\n",
    "        data.append(participant_data)\n",
    "\n",
    "    top_regions = frozenset(\n",
    "        region for region, _ in sorted(\n",
    "            region_percentages.items(), key=lambda pair: pair[1], reverse=True\n",
    "        )[:top_n]\n",
    "    )\n",
    "\n",
    "    for participant_data in data:\n",
    "        region = participant_data[\"region\"]\n",
    "        if region not in top_regions:\n",
    "            participant_data[\"region\"] = \"other\"\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "    df = df.groupby([\"generation\", \"region\"])[\"dialect_percentage\"].mean().unstack()\n",
    "    df = df.sort_values(by=\"generation\")\n",
    "    df = df.replace(np.nan, 0.0)\n",
    "    return df\n",
    "\n",
    "def get_region_dialects_df() -> pd.DataFrame:\n",
    "    region_dialects_df = generational_dialect_percentages()\n",
    "    region_dialects_df = region_dialects_df[region_dialects_df.columns]\n",
    "    region_dialects_df = pd.DataFrame(region_dialects_df.apply(partial(round, ndigits=2), axis=1))\n",
    "    region_dialects_df = region_dialects_df.loc[:, (region_dialects_df != 0).sum() >= 2]\n",
    "    return region_dialects_df\n",
    "\n",
    "def generation_grouped_percentages(region_dialects_df: pd.DataFrame) -> dict[str, Any]:\n",
    "    percentages = {\n",
    "        generation.name: {}\n",
    "        for generation in Generation.create_mapping()\n",
    "    }\n",
    "\n",
    "    for i, row in region_dialects_df.iterrows():\n",
    "        values = {\n",
    "            key: float(cast(np.float64, value)) \n",
    "            for key, value in dict(row).items()\n",
    "        }\n",
    "        percentages[cast(str, i)] = values\n",
    "\n",
    "    return percentages\n",
    "\n",
    "def calculate_regional_deltas(region_dialects_df: pd.DataFrame) -> dict[str, float]:\n",
    "    def percentage_delta(percentages: list[float]) -> float:\n",
    "        delta = 0\n",
    "        prev = percentages[0]\n",
    "        for percentage in percentages[1:]:\n",
    "            delta += percentage - prev\n",
    "            prev = percentage\n",
    "        return delta\n",
    "    \n",
    "    percentages_per_region = defaultdict(list)\n",
    "    for _, row in region_dialects_df.iterrows():\n",
    "        for region, percentage in dict(row).items():\n",
    "            percentages_per_region[region].append(float(percentage))  # type: ignore\n",
    "\n",
    "    result = {}\n",
    "    for region, values in percentages_per_region.items():\n",
    "        values = list(filter(None, values))\n",
    "        result[region] = round_precise(percentage_delta(values), 2)\n",
    "\n",
    "    return result\n",
    "\n",
    "data = create_json_data()\n",
    "data[\"metadata\"][\"title\"] = \"Changes of percentage of dialect words spoken over generations\"\n",
    "data[\"data\"] = calculate_regional_deltas(get_region_dialects_df())\n",
    "export_json_data(\"dialect_delta_percentages.json\", data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8148cc5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections.abc import Generator\n",
    "\n",
    "from eda.language import AttributedWord\n",
    "from eda.models import Conversation, Participant\n",
    "from eda.parsing import ParticipantLines\n",
    "from eda.utils import round_precise\n",
    "\n",
    "UNKNOWN_EDUCATION = \"N/A\"\n",
    "EDUCATION_RANKINGS = [\n",
    "    \"elem\", \"dip_tec_prof\", \"dip_lic\", \"laurea in corso\", \"laurea\", \"med\", \"phd\"\n",
    "]\n",
    "GENERATION_ORDER = [\n",
    "    Generation.BOOMERS, Generation.X, Generation.Y, Generation.Z\n",
    "]\n",
    "\n",
    "def participant_dialect_percentage(participant: Participant) -> int | float:\n",
    "    participant_lines = conversations.participant_lines(participant)\n",
    "    dialect_words = total_words = 0\n",
    "    for word in generate_participant_words(participant_lines):\n",
    "        dialect_words += word.is_dialect(strict=False)\n",
    "        total_words += 1\n",
    "\n",
    "    return round_precise(dialect_words / total_words * 100, 2)\n",
    "\n",
    "def approximate_participant_age(participant: Participant) -> int | float:\n",
    "    if participant.age_range.is_oldest():\n",
    "        return participant.age_range.oldest_age\n",
    "    else:\n",
    "        return (participant.age_range.youngest_age + participant.age_range.oldest_age) / 2\n",
    "\n",
    "def conversation_generation(conversation: Conversation) -> Generation:\n",
    "    # Lower median\n",
    "    generations = [participant.generation for participant in conversation.participants]\n",
    "    counts = Counter(generations)\n",
    "    most_common, count = counts.most_common(1)[0]\n",
    "    if count > len(generations) / 2:\n",
    "        return most_common\n",
    "\n",
    "    generations.sort(key=GENERATION_ORDER.index)\n",
    "    median_index = (len(generations) - 1) // 2\n",
    "    return generations[median_index]\n",
    "\n",
    "def conversation_average_age(conversation: Conversation) -> float:\n",
    "    age_ranges = list(map(approximate_participant_age, conversation.participants))\n",
    "    return round_precise(sum(age_ranges) / len(age_ranges))\n",
    "    \n",
    "def conversation_educational_background(conversation: Conversation) -> str:\n",
    "    # Lowest background\n",
    "    backgrounds = [participant.degree for participant in conversation.participants]\n",
    "    backgrounds = list(filter(lambda b: b != \"N/A\", backgrounds))\n",
    "    counts = Counter(backgrounds)\n",
    "    most_common, count = counts.most_common(1)[0]\n",
    "    if count > len(backgrounds) / 2:\n",
    "        return most_common\n",
    "    \n",
    "    return min(backgrounds, key=EDUCATION_RANKINGS.index)\n",
    "    \n",
    "def conversation_participant_data(conversation: Conversation) -> dict[str, Any]:\n",
    "    n_participants = len(conversation.participants)\n",
    "    dialect_percentages = list(map(participant_dialect_percentage, conversation.participants))\n",
    "\n",
    "    result = {}\n",
    "    result[\"n_participants\"] = n_participants\n",
    "    result[\"dialect_percentage\"] = round_precise(sum(dialect_percentages) / n_participants)\n",
    "\n",
    "    sort = {}\n",
    "    sort[\"average_approximate_age\"] = conversation_average_age(conversation)\n",
    "    sort[\"generation\"] = conversation_generation(conversation).name\n",
    "    sort[\"region\"] = conversation.macro_region.name.lower()\n",
    "    sort[\"educational_background\"] = conversation_educational_background(conversation)\n",
    "    result[\"sort\"] = sort\n",
    "    return result\n",
    "\n",
    "conversations.read_all()\n",
    "\n",
    "data = create_json_data()\n",
    "data[\"metadata\"][\"title\"] = \"Dialect percentages based on other participant statistics\"\n",
    "data[\"data\"] = {\n",
    "    conversation.code: conversation_participant_data(conversation) \n",
    "    for conversation in sorted(conversations, key=lambda c: c.code)\n",
    "}\n",
    "export_json_data(\"dialect_comparisons.json\", data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
