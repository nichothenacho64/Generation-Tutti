{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ba750a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import sys\n",
    "from typing import Any\n",
    "\n",
    "sys.path.append(os.path.abspath(\"..\"))\n",
    "\n",
    "from eda.models import Generation\n",
    "from eda.parsing import Conversations, Participants\n",
    "from eda.utils import FOLDER_DIR\n",
    "\n",
    "DATA_PATH = FOLDER_DIR / \"data\"\n",
    "\n",
    "def export_json_data(name: str, data: dict[str, Any]):\n",
    "    with DATA_PATH.joinpath(name).open(\"w\") as fp:\n",
    "        json.dump(data, fp, indent=4, ensure_ascii=False)\n",
    "\n",
    "\n",
    "def create_json_data() -> dict[str, Any]:\n",
    "    return {\"metadata\": {}, \"data\": {}}\n",
    "\n",
    "participants = Participants()\n",
    "conversations = Conversations(participants)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17108b93",
   "metadata": {},
   "source": [
    "### Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "81f5cb02",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversations.read_all(parallel=True, load_sentiments=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bcca77c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from collections.abc import Iterable\n",
    "\n",
    "from eda.models import ConversationLine\n",
    "from eda.sentiments import SentimentType\n",
    "from eda.utils import round_precise\n",
    "\n",
    "\n",
    "def sentiments_from_lines(\n",
    "    lines: Iterable[ConversationLine], exclude_true_neutrals: bool = False\n",
    ") -> tuple[list[str], list[float]]:\n",
    "    sentiments = Counter()\n",
    "\n",
    "    for line in lines:\n",
    "        if not line.sentiments.has_loaded_scores():\n",
    "            continue\n",
    "        if not line.sentiments.has_scores():\n",
    "            continue\n",
    "        if exclude_true_neutrals and line.sentiments.neutral == 1.0:\n",
    "            continue\n",
    "        sentiments += line.sentiments.score_counts\n",
    "\n",
    "    total_sum = sum(sentiments.values())\n",
    "    names, scores = zip(\n",
    "        *(\n",
    "            (\n",
    "                SentimentType(sentiment_name).display_name,\n",
    "                round_precise(score / total_sum * 100, 2),\n",
    "            )\n",
    "            for sentiment_name, score in sentiments.items()\n",
    "        )\n",
    "    )\n",
    "    return list(names), list(scores)\n",
    "\n",
    "\n",
    "def sentiment_percentages() -> dict[str, dict[str, Any]]:\n",
    "    lines_by_generation = Generation.create_mapping()\n",
    "    for conversation in conversations:\n",
    "        for line in conversation.lines:\n",
    "            generation = line.participant.generation\n",
    "            lines_by_generation[generation].append(line)\n",
    "\n",
    "    sentiment_names = None\n",
    "    generation_names = []\n",
    "    data = {}\n",
    "\n",
    "    for generation, lines in lines_by_generation.items():\n",
    "        generation_names.append(generation.name)\n",
    "        sentiment_names, scores = sentiments_from_lines(\n",
    "            lines, exclude_true_neutrals=True\n",
    "        )\n",
    "\n",
    "        total = sum(scores)\n",
    "        proportions = [\n",
    "            (sentiment_name, score / total * 100)\n",
    "            for sentiment_name, score in zip(sentiment_names, scores)\n",
    "        ]\n",
    "        \n",
    "        data[generation.name] = dict(proportions)\n",
    "\n",
    "    return data\n",
    "\n",
    "data = create_json_data()\n",
    "data[\"metadata\"][\"title\"] = \"Sentiment percentages per generation\"\n",
    "data[\"data\"] = sentiment_percentages()\n",
    "export_json_data(\"sentiment_percentages.json\", data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "97be9240",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversations.read_all(parallel=True, load_prosodic=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7cfb0ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from collections import defaultdict\n",
    "from collections.abc import Generator\n",
    "\n",
    "from eda.models import ConversationLine, Participant\n",
    "\n",
    "\n",
    "def human_name_from_snake_case(name: str) -> str:\n",
    "    return \" \".join(name.split(\"_\")).capitalize()\n",
    "\n",
    "\n",
    "def filter_prosodic_attributes(\n",
    "    line: ConversationLine,\n",
    ") -> Generator[tuple[str, str]]:\n",
    "    yield from (\n",
    "        (name, value) for name, value in vars(line).items() \n",
    "        if name.endswith(\"phrases\")\n",
    "        and name != \"overlapping_phrases\"\n",
    "    )\n",
    "\n",
    "\n",
    "def prosodic_frequencies(participant: Participant) -> dict[str, float]:\n",
    "    participant_lines = conversations.participant_lines(participant)\n",
    "    participant_data = defaultdict(int)\n",
    "    n_lines = 0\n",
    "    for line in participant_lines:\n",
    "        for name, value in filter_prosodic_attributes(line):\n",
    "            participant_data[name] += len(value)\n",
    "        n_lines += 1\n",
    "\n",
    "    norm_participant_data = {\n",
    "        key: value / n_lines * 100 for key, value in participant_data.items()\n",
    "    }\n",
    "    return norm_participant_data\n",
    "\n",
    "\n",
    "def prosodic_counts_by_generation(data: dict[Generation, Any]) -> dict[str, dict[str, float]]:\n",
    "    for participant in participants:\n",
    "        generation = participant.generation\n",
    "        data[generation].append(prosodic_frequencies(participant))\n",
    "\n",
    "    result = {}\n",
    "    for generation, participants_frequencies in data.items():\n",
    "        total_generation_counts = Counter()\n",
    "        for participant_frequencies in participants_frequencies:\n",
    "            total_generation_counts += Counter(participant_frequencies)\n",
    "\n",
    "        result[generation.name] = {\n",
    "            human_name_from_snake_case(prosodic_feature).replace(\" phrases\", \"\"):\n",
    "            round_precise(count / len(participants_frequencies))\n",
    "            for prosodic_feature, count in total_generation_counts.items()\n",
    "        }\n",
    "    return result\n",
    "\n",
    "\n",
    "generation_map = Generation.create_mapping()\n",
    "counts = prosodic_counts_by_generation(generation_map)\n",
    "data = create_json_data()\n",
    "data[\"metadata\"][\"title\"] = \"Average prosodic feature frequency per line by generation\"\n",
    "data[\"data\"] = counts\n",
    "export_json_data(\"prosodic_features.json\", data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d68d7f",
   "metadata": {},
   "source": [
    "### Question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d71f11d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversations.read_all(parallel=True, load_tagged=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "392beff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional\n",
    "\n",
    "from eda.language import TaggedText\n",
    "\n",
    "TOP_N_LEMMAS = 10\n",
    "MIN_WORD_OCCURRENCES = 3\n",
    "ALLOWED_POS_VALUES = None\n",
    "PER_WORDS = 2500\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class PosTaggedLemma:\n",
    "    lemma: str\n",
    "    pos_name: str\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class GenerationLemmaInfo:\n",
    "    pt_lemmas: list[PosTaggedLemma]\n",
    "    lemma_counts: list[int]\n",
    "    top_counter: Counter\n",
    "    n_total_words: int\n",
    "\n",
    "\n",
    "def top_lemmas_by_generation(\n",
    "    conversations,\n",
    "    *,\n",
    "    top_n: Optional[int] = None,\n",
    "    min_lemma_length: int = 3,\n",
    "    min_word_occurences: int = 3,\n",
    "    allowed_pos_values: Optional[set] = None,\n",
    ") -> dict[str, GenerationLemmaInfo]:\n",
    "    text_by_generation: dict[str, list[TaggedText]] = {\n",
    "        key.name: value for key, value in Generation.create_mapping().items()\n",
    "    }\n",
    "\n",
    "    for conversation in conversations:\n",
    "        for line in conversation:\n",
    "            generation_name: str = line.participant.generation.name\n",
    "            text_by_generation[generation_name].extend(line.tagged)\n",
    "\n",
    "    for generation, words in text_by_generation.items():\n",
    "        word_counts = Counter(words)\n",
    "        words = list({word for word in words if word_counts[word] >= min_word_occurences})\n",
    "        text_by_generation[generation] = words\n",
    "\n",
    "    lemmas_by_generation = {}\n",
    "    for generation, words in text_by_generation.items():\n",
    "        words_by_lemma = defaultdict(list)\n",
    "        for word in words:\n",
    "            if len(word.lemma) < min_lemma_length:\n",
    "                continue\n",
    "            if allowed_pos_values is not None and word.pos not in allowed_pos_values:\n",
    "                continue\n",
    "            pt_lemma = PosTaggedLemma(word.lemma, word.pos_name)\n",
    "            words_by_lemma[pt_lemma].append(word)\n",
    "\n",
    "        counts = Counter({\n",
    "            pt_lemma: len(group) for pt_lemma, group in words_by_lemma.items()\n",
    "        })\n",
    "\n",
    "        lemmas, frequencies = zip(*counts.most_common(top_n))\n",
    "        lemmas_by_generation[generation] = GenerationLemmaInfo(\n",
    "            list(lemmas), \n",
    "            list(frequencies), \n",
    "            counts,\n",
    "            len(words)\n",
    "        )\n",
    "\n",
    "    return lemmas_by_generation\n",
    "\n",
    "def important_lemmas() -> dict[str, Any]:\n",
    "    top_lemmas = top_lemmas_by_generation(\n",
    "        conversations, top_n=None, allowed_pos_values=ALLOWED_POS_VALUES\n",
    "    )\n",
    "\n",
    "    result: dict[str, dict[str, float]] = {key.name: {} for key in Generation.create_mapping()}\n",
    "    for generation_name, info in top_lemmas.items():\n",
    "        for ptl, count in zip(info.pt_lemmas, info.lemma_counts):\n",
    "            lemma_frequency = round_precise(count / info.n_total_words * PER_WORDS)\n",
    "            result[generation_name][ptl.lemma] = lemma_frequency\n",
    "\n",
    "    original_result = result\n",
    "    top_lemmas_result = {}\n",
    "    for generation_name, info in top_lemmas.items():\n",
    "        top_counts = info.top_counter.most_common(TOP_N_LEMMAS)\n",
    "        allowed = {ptl.lemma for ptl, _ in top_counts}\n",
    "        top_lemmas_result[generation_name] = {\n",
    "            lemma: result[generation_name][lemma]\n",
    "            for lemma in allowed\n",
    "        }\n",
    "\n",
    "    result = top_lemmas_result\n",
    "    for generation_name, other_generation in itertools.product(result, repeat=2):\n",
    "        if generation_name == other_generation:\n",
    "            continue\n",
    "\n",
    "        for lemma in result[other_generation]:\n",
    "            if lemma not in result[generation_name]:\n",
    "                result[generation_name][lemma] = original_result[generation_name][lemma]\n",
    "\n",
    "        result[generation_name] = dict(\n",
    "            sorted(result[generation_name].items(), key=lambda pair: pair[1], reverse=True)\n",
    "        )\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "data = create_json_data()\n",
    "data[\"metadata\"][\"title\"] = \"Top lemmas by generation\"\n",
    "data[\"metadata\"][\"top_n_lemmas\"] = TOP_N_LEMMAS\n",
    "data[\"metadata\"][\"per_n_words\"] = PER_WORDS\n",
    "data[\"metadata\"][\"min_word_occurrences\"] = MIN_WORD_OCCURRENCES\n",
    "data[\"data\"] = important_lemmas()\n",
    "export_json_data(\"top_lemmas.json\", data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a451a5f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from functools import partial\n",
    "\n",
    "themes_by_generation = defaultdict(partial(defaultdict, partial(defaultdict, list)))\n",
    "themes_by_code = json.loads(DATA_PATH.joinpath(\"ml_gen_themes_by_code.json\").read_text())\n",
    "theme_counts = Counter()\n",
    "totals = Counter()\n",
    "\n",
    "for conversation_data in themes_by_code.values():\n",
    "    generation_values = conversation_data[\"values\"]\n",
    "    generation_name = conversation_data[\"generation\"]\n",
    "\n",
    "    for value in generation_values:\n",
    "        themes = value[\"themes\"]\n",
    "        existing_lemmas = value[\"lemmas\"]\n",
    "\n",
    "        for theme in themes:\n",
    "            entry = themes_by_generation[generation_name][theme]\n",
    "            entry[\"lemmas\"].extend(existing_lemmas)\n",
    "            theme_counts[(generation_name, theme)] += 1\n",
    "\n",
    "        totals[generation_name] += 1\n",
    "\n",
    "top_lemmas = {\n",
    "    generation: lemmas\n",
    "    for generation, lemmas in json.loads(\n",
    "        DATA_PATH.joinpath(\"top_lemmas.json\").read_text()\n",
    "    )[\"data\"].items()\n",
    "}\n",
    "\n",
    "for generation_name, generation_values in themes_by_generation.items():\n",
    "    for theme, theme_data in generation_values.items():\n",
    "        unique_themes = frozenset(theme_data[\"lemmas\"])\n",
    "        theme_data[\"match\"] = round_precise(\n",
    "            theme_counts[(generation_name, theme)] / totals[generation_name] * 100\n",
    "        )\n",
    "        theme_data[\"filtered_lemmas\"] = list(\n",
    "            filter(top_lemmas[generation_name].__contains__, top_lemmas[generation_name])\n",
    "        )\n",
    "        theme_data[\"lemmas\"] = sorted(unique_themes)\n",
    "\n",
    "data = create_json_data()\n",
    "data[\"metadata\"][\"title\"] = \"Themes by generation\"\n",
    "data[\"data\"] = themes_by_generation\n",
    "export_json_data(\"themes_by_generation.json\", data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d08f9a71",
   "metadata": {},
   "source": [
    "### Question 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fb11af82",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from collections.abc import Callable, Generator\n",
    "from functools import cache\n",
    "\n",
    "from eda.language import AttributedWord\n",
    "from eda.models import MacroRegion, ParticipantLines\n",
    "\n",
    "\n",
    "def participant_macro_region(participant: Participant) -> MacroRegion:\n",
    "    conversation = conversations.conversation(participant.conversation_code)\n",
    "    return conversation.macro_region\n",
    "\n",
    "\n",
    "def generate_participant_words(lines: ParticipantLines) -> Generator[AttributedWord]:\n",
    "    for line in lines:\n",
    "        yield from filter(lambda word: word.is_linguistic, line.normalised_words)\n",
    "\n",
    "@cache\n",
    "def participants_dialect_percentages(\n",
    "    *, rounder: Callable[[int | float, int], int | float] = round_precise\n",
    ") -> list[int | float]:\n",
    "    percentages = []\n",
    "\n",
    "    for participant in participants:\n",
    "        participant_lines = conversations.participant_lines(participant)\n",
    "        dialect_words = total_words = 0\n",
    "        for word in generate_participant_words(participant_lines):\n",
    "            dialect_words += word.is_dialect(strict=False)\n",
    "            total_words += 1\n",
    "\n",
    "        percentange_of_dialect_words = rounder(dialect_words / total_words * 100, 2)\n",
    "        percentages.append(percentange_of_dialect_words)\n",
    "\n",
    "    return percentages\n",
    "\n",
    "def labeled_percentages() -> list[dict[str, Any]]:\n",
    "    result = {\n",
    "        key.name: value \n",
    "        for key, value in Generation.create_mapping().items()\n",
    "    }\n",
    "    \n",
    "    region_percentages = defaultdict(float)\n",
    "\n",
    "    dialect_percentages = iter(participants_dialect_percentages())\n",
    "    for participant in participants:\n",
    "        region = participant.geographic_origin\n",
    "        percentage = next(dialect_percentages)\n",
    "        participant_data = {\n",
    "            \"region\": region,\n",
    "            \"dialect_percentage\": percentage,\n",
    "            \"macro_region\": participant.macro_region.name.lower()\n",
    "        }\n",
    "        region_percentages[region] += percentage\n",
    "        result[participant.generation.name].append(participant_data)\n",
    "    return result  # type: ignore\n",
    "\n",
    "data = create_json_data()\n",
    "data[\"metadata\"][\"title\"] = \"Dialect word percentages\"\n",
    "data[\"data\"] = labeled_percentages()\n",
    "export_json_data(\"dialect_percentages.json\", data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9303c8c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from typing import Optional, cast\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def generational_dialect_percentages(top_n: Optional[int] = None) -> pd.DataFrame:\n",
    "    data = []\n",
    "    region_percentages = defaultdict(float)\n",
    "\n",
    "    dialect_percentages = iter(participants_dialect_percentages())\n",
    "    for participant in participants:\n",
    "        region = participant.geographic_origin\n",
    "        percentage = next(dialect_percentages)\n",
    "        participant_data = {\n",
    "            \"generation\": participant.generation.name,\n",
    "            \"dialect_percentage\": percentage,\n",
    "            \"region\": region\n",
    "        }\n",
    "        region_percentages[region] += percentage\n",
    "        data.append(participant_data)\n",
    "\n",
    "    top_regions = frozenset(\n",
    "        region for region, _ in sorted(\n",
    "            region_percentages.items(), key=lambda pair: pair[1], reverse=True\n",
    "        )[:top_n]\n",
    "    )\n",
    "\n",
    "    for participant_data in data:\n",
    "        region = participant_data[\"region\"]\n",
    "        if region not in top_regions:\n",
    "            participant_data[\"region\"] = \"other\"\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "    df = df.groupby([\"generation\", \"region\"])[\"dialect_percentage\"].mean().unstack()\n",
    "    df = df.sort_values(by=\"generation\")\n",
    "    df = df.replace(np.nan, 0.0)\n",
    "    return df\n",
    "\n",
    "def get_region_dialects_df() -> pd.DataFrame:\n",
    "    region_dialects_df = generational_dialect_percentages()\n",
    "    region_dialects_df = region_dialects_df[region_dialects_df.columns]\n",
    "    region_dialects_df = pd.DataFrame(region_dialects_df.apply(partial(round, ndigits=2), axis=1))\n",
    "    region_dialects_df = region_dialects_df.loc[:, (region_dialects_df != 0).sum() >= 2]\n",
    "    return region_dialects_df\n",
    "\n",
    "def generation_grouped_percentages(region_dialects_df: pd.DataFrame) -> dict[str, Any]:\n",
    "    percentages = {\n",
    "        generation.name: {}\n",
    "        for generation in Generation.create_mapping()\n",
    "    }\n",
    "\n",
    "    for i, row in region_dialects_df.iterrows():\n",
    "        values = {\n",
    "            key: float(cast(np.float64, value)) \n",
    "            for key, value in dict(row).items()\n",
    "        }\n",
    "        percentages[cast(str, i)] = values\n",
    "\n",
    "    return percentages\n",
    "\n",
    "def calculate_regional_deltas(region_dialects_df: pd.DataFrame) -> dict[str, float]:\n",
    "    def percentage_delta(percentages: list[float]) -> float:\n",
    "        delta = 0\n",
    "        prev = percentages[0]\n",
    "        for percentage in percentages[1:]:\n",
    "            delta += percentage - prev\n",
    "            prev = percentage\n",
    "        return delta\n",
    "    \n",
    "    percentages_per_region = defaultdict(list)\n",
    "    for _, row in region_dialects_df.iterrows():\n",
    "        for region, percentage in dict(row).items():\n",
    "            percentages_per_region[region].append(float(percentage))  # type: ignore\n",
    "\n",
    "    result = {}\n",
    "    for region, values in percentages_per_region.items():\n",
    "        values = list(filter(None, values))\n",
    "        result[region] = round_precise(percentage_delta(values), 2)\n",
    "\n",
    "    return result\n",
    "\n",
    "data = create_json_data()\n",
    "data[\"metadata\"][\"title\"] = \"Changes of percentage of dialect words spoken over generations\"\n",
    "data[\"data\"] = calculate_regional_deltas(get_region_dialects_df())\n",
    "export_json_data(\"dialect_delta_percentages.json\", data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8148cc5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from eda.models import Conversation, Participant\n",
    "from eda.utils import round_precise\n",
    "\n",
    "UNKNOWN_EDUCATION = \"N/A\"\n",
    "EDUCATION_RANKINGS = [\n",
    "    \"elem\", \"dip_tec_prof\", \"dip_lic\", \"laurea in corso\", \"laurea\", \"med\", \"phd\"\n",
    "]\n",
    "GENERATION_ORDER = [\n",
    "    Generation.BOOMERS, Generation.X, Generation.Y, Generation.Z\n",
    "]\n",
    "\n",
    "def approximate_participant_age(participant: Participant) -> int | float:\n",
    "    if participant.age_range.is_oldest():\n",
    "        return participant.age_range.oldest_age\n",
    "    else:\n",
    "        return (participant.age_range.youngest_age + participant.age_range.oldest_age) / 2\n",
    "\n",
    "def conversation_generation(conversation: Conversation) -> Generation:\n",
    "    # Lower median\n",
    "    generations = [participant.generation for participant in conversation.participants]\n",
    "    counts = Counter(generations)\n",
    "    most_common, count = counts.most_common(1)[0]\n",
    "    if count > len(generations) / 2:\n",
    "        return most_common\n",
    "\n",
    "    generations.sort(key=GENERATION_ORDER.index)\n",
    "    median_index = (len(generations) - 1) // 2\n",
    "    return generations[median_index]\n",
    "\n",
    "def conversation_average_age(conversation: Conversation) -> int | float:\n",
    "    age_ranges = list(map(approximate_participant_age, conversation.participants))\n",
    "    return round_precise(sum(age_ranges) / len(age_ranges))\n",
    "    \n",
    "def conversation_dialect_percentage(conversation: Conversation) -> float:\n",
    "    def generate_conversation_words():\n",
    "        for line in conversation:\n",
    "            yield from filter(lambda word: word.is_linguistic, line.normalised_words)\n",
    "\n",
    "    total_words = dialect_words = 0\n",
    "    for word in generate_conversation_words():\n",
    "        dialect_words += word.is_dialect(strict=False)\n",
    "        total_words += 1\n",
    "    return round_precise(dialect_words / total_words * 100)\n",
    "\n",
    "def conversation_educational_background(conversation: Conversation) -> str:\n",
    "    # Lowest background\n",
    "    backgrounds = [participant.degree for participant in conversation.participants]\n",
    "    backgrounds = list(filter(lambda b: b != \"N/A\", backgrounds))\n",
    "    counts = Counter(backgrounds)\n",
    "    most_common, count = counts.most_common(1)[0]\n",
    "    if count > len(backgrounds) / 2:\n",
    "        return most_common\n",
    "    \n",
    "    return min(backgrounds, key=EDUCATION_RANKINGS.index)\n",
    "    \n",
    "def conversation_participant_data(conversation: Conversation) -> dict[str, Any]:\n",
    "    n_participants = len(conversation.participants)\n",
    "\n",
    "    result = {}\n",
    "    result[\"n_participants\"] = n_participants\n",
    "    result[\"dialect_percentage\"] = conversation_dialect_percentage(conversation)\n",
    "    result[\"average_approximate_age\"] = conversation_average_age(conversation)\n",
    "\n",
    "    sort = {}\n",
    "    sort[\"generation\"] = conversation_generation(conversation).name\n",
    "    sort[\"macro_region\"] = conversation.macro_region.name.lower()\n",
    "    sort[\"educational_background\"] = conversation_educational_background(conversation)\n",
    "    result[\"sort\"] = sort\n",
    "    return result\n",
    "\n",
    "conversations.read_all()\n",
    "\n",
    "data = create_json_data()\n",
    "data[\"metadata\"][\"title\"] = \"Dialect percentages based on other participant statistics\"\n",
    "data[\"data\"] = {\n",
    "    conversation.code: conversation_participant_data(conversation) \n",
    "    for conversation in sorted(conversations, key=lambda c: c.code)\n",
    "}\n",
    "export_json_data(\"dialect_comparisons.json\", data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
